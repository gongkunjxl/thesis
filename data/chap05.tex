\chapter{面向多计算框架的容器云平台研发与调度实验}
提出MRWS调度方案优化OpenShift容器云平台底层容器编排引擎Kubernetes调度流程和调度算法后，需要对其资源利用率和负载均衡性进行评估，测试新调度方案下集群的调度性能。本章实验主要包括如下几个部分:
\begin{enumerate}[(1)]
	\item 在容器云仿真平台上ContainerCloudSim进行大规模容器应用调度仿真、对比Kubernetes默认的Default算法、Random、FirstFit和MRWS调度算法在资源利用率和负载均衡性方面的优劣。
	\item 基于开源OpenShift Origin和存储系统Paladin Storage研发了面向多计算框架的容器云平台Paladin，该平台是一个集海量数据存储管理、多计算框架快速部署、调度优化、用户注册、按需服务等多功能的PaaS平台。
	\item 在Paladin上支持十多种大数据处理框架如Hadoop、Spark、Storm等，将其打包成镜像文件，Push到仓库中，用户可以快速构建大数据处理环境。
	\item 在Paladin容器云平台上设计并实现MRWS方法的调度器，使用多计算框架容器应用混合部署进行调度测试，对比几种调度方案的服务性能。
\end{enumerate}

\section{ContainerCloudSim仿真MRWS调度方案}
自研的面向多计算框架的容器云平台Paladin部署在小规模物理集群上，不能进行调度方案的大规模调度测试和分析。为评估MRWS调度方案的性能和可靠性，需要在容器云仿真平台ContainerCloudSim~\cite{Piraghaj2016ContainerCloudSim}上进行大规模的仿真实验。首先构建仿真实验环境，然后开发Kubernetes默认的Default算法、Random、FirstFit和MRWS调度算法，使用大规模负载进行测试分析。

\subsection{ContainerCloudSim容器云仿真平台}
\subsubsection{CloudSim云仿真平台}
容器和容器编排技术的逐渐成熟推动了容器云的飞速发展，在容器云计算系统中，为评测资源调度策略和集群服务性能，一个容器云仿真平台变得异常重要。调度方案经过仿真平台大量测试和对比分析，不仅可以节约开发时间，也能避免资源浪费和减少试错成本。针对不同的应用场景和新调度方案，如果实际部署大规模容器云平台进行性能测试分析，绝大部分小公司和研究人员并不具备这种条件。在传统的云计算模式下，应用服务的组成、供应、配置和部署条件较为复杂，当用户需求和系统配置动态变化时，评估一个调度策略以及工作负载是相当困难的，一个优秀的云计算平台模拟器可以很好解决这个问题。云计算模拟器通过控制环境变量和重复试验可以加速理论研究和开发过程，根据需求和应用场景不同，各大公司和研究机构推出了一系列云计算仿真平台。

MDCSim~\cite{Lim2009MDCSim}是一个全面、灵活、可扩展的多层云计算仿真平台，整个仿真平台分为通信层、内核层和用户层。通信层用于模拟集群内部通信、内核层用于模拟调度和分析系统性能、用户层用于模拟各种实际应用。该模拟器能够根据底层硬件的不同特点进行混合建模，用于评估数据中心的能耗，让用户在保持低功耗的同时实现集群服务性能的提升。GroudSim~\cite{Ostermann2011GroudSim}是一个基于Java的模拟器，用于模拟科学应用在网格和云设施上执行问题，应用执行完成后给用户提供了基础的数据统计和分析功能。NetworkCloudSim~\cite{Garg2012NetworkCloudSim}用于解决网络模拟问题，弥补其他模拟器对网络细节关注不足，支持MPI和工作流。TeachCloud~\cite{Jararweh2013TeachCloud}用于对MapReduce应用建模并集成一个负载生成器，提供图形化的接口和实现定制的网络拓扑结构。CloudSim~\cite{Calheiros2009CloudSim}是墨尔本大学Gridbus项目推出的云计算仿真软件，既能对系统性能和应用服务模拟、仿真、测试，也能用于评估资源调度策略的优劣。

CloudSim是一个开源的仿真软件，最大特点就是提供一个虚拟化引擎，帮助数据中心建立和管理各种虚拟化服务。支持大规模云计算资源管理和调度模拟，将数据中心的资源虚拟化为资源池，CloudSim的分层体系架构图如5.1所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{cloudsim-layer}
	\caption{CloudSim分层架构图}
\end{figure}

\subsubsection{ContaienrCloudSim容器云仿真平台}
随着容器技术的迅猛发展，CaaS(Congtianer as a Service)作为一种新型的服务模式变得越来越普遍。上述介绍的各种云计算仿真平台以及早期的CloudSim版本并不支持容器仿真，一个容器应用仿真平台变得越加紧迫。为促进容器云发展，缩短新方法研发时间，墨尔本大学的研究人员利用CloudSim虚拟化的特点，在其基础上开发了ContainerCloudSim，专门用于数据中心容器应用的仿真。在最新的CloudSim-4.0上已经集成ContainerCloudSim组件，提供Docker容器应用仿真支持，ContainerCloudSim与CloudSim生态圈关系如图5.2所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{container-cloudsim}
	\caption{ContainerCloudSim与CloudSim生态圈}
\end{figure}
集成ContainerCloudSim的CloudSim-4.0云平台模拟器已完整支持容器应用的云计算仿真，新版本仿真平台特点如下:
\begin{enumerate}[(1)]
	\item 支持大规模云计算建模和仿真。
	\item 支持虚拟机调度策略，提供服务器和主机虚拟化。
	\item 支持大规模容器应用的建模和调度仿真。
	\item 支持计算资源能量感知的建模和仿真。
	\item 支持网络拓扑结构和消息传递应用建模和仿真。
	\item 支持元素的动态增加、停止和恢复的仿真。
	\item 支持混合云的建模和仿真。
\end{enumerate}

在ContainerCloudSim部分，提供容器、VM、Host以及数据中心CPU、内存和存储等资源的管理。还实现了系统动态监控、容器应用执行控制以及容器虚拟机的供应等功能。容器模拟器需要给研究人员提供容器调度方案接口，以及各种调度算法之间的对比和评测，容器调度策略决定容器如何被调度到虚拟机上。调度算法的能耗问题也是容器模拟器关注的重点，需要提供各种算法的能耗度量，此外，还有容器的合并和迁移等功能。最后，模拟器要能够支持容器的扩展，在CaaS中，容器的数量要远远多于虚拟机。ContainerCloudSim是在CloudSim基础上开发而来，也是一个分层的架构，整体架构如下所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{containercloudsim-layer}
	\caption{ContainerCloudSim分层架构图}
\end{figure}
从下至上依次分为数据中心管理服务层、能耗监控服务层、资源分配服务层、资源管理服务层、虚拟机生命周期管理服务层、容器生命周期管理服务层和负载管理服务层等。每个层次的大致作用如下:
\begin{enumerate}
	\item 负载管理服务层关注于客户端应用的注册、部署、调度、应用层的性能以及容器应用的健康监控。
	\item 容器生命周期服务管理层负责容器生命周期管理，包括创建容器、注册容器到系统中、启动、停止、重启、从一个主机迁移到另一个主机以及容器销毁。除此之外，还负责执行和管理在容器中任务，监控任务资源利用率。
	\item 虚拟机生命周期管理服务层负责虚拟机的管理包括创建、启动、停止、重启、销毁、迁移以及资源利用率监控。
	\item 资源管理服务层负责容器在满足资源需求和软件环境的虚拟机上创建，由容器调度、虚拟机调度和容器合并组成。容器调度根据容器调度策略调度容器到虚拟机、虚拟机调度根据虚拟机调度策略调度虚拟机到主机，合并策略通过合并容器减少主机需求，最小化资源碎片。
	\item 资源分配服务层负责管理虚拟机和容器的资源分配，由容器分配服务和虚拟机分配服务构成。容器分配服务负责虚拟机资源分配给容器，虚拟机分配服务负责主机资源分配给虚拟机。
	\item 能耗监控服务负责数据中心主机能耗监控，构建必要的能耗模型。
	\item 数据中心管理服务负责管理数据中心资源，主机开关机和资源监控。
\end{enumerate}
至此，ContainerCloudSim容器云仿真平台介绍完毕，具体的仿真执行流程和代码实现可以参见其开源的源代码。

\subsection{MRWS资源利用率实验}
在ContainerCloudSim容器云仿真平台上的ContainerPlacementPolicy库下开发MRWS调度算法、Kubernetes默认的Default算法，该类自带Random和FirstFit算法，对比四种调度算法下集群资源的利用率。该容器云仿真平台影响调度因素较多，需要根据实际需要进行一些必要的条件控制。

MRWS算法和Kubernetes的Default算法在预选阶段使用的筛选规则相同，评分阶段除空闲资源评分函数和平衡函数外，其他评分函数相同。因此，假设其他评分函数和外部条件都相同的情况下，只需模拟比较MRWS算法和Default算法的空闲资源评分函数和资源平衡评分函数。实验目的在于对比大规模容器调度下各种调度算法的集群资源利用率和负载均衡性，云计算中虚拟机的调度已有相当多的研究，不是本文的研究方向。首先，需要对仿真平台的部分条件加以限制，假设ContainerCloudSim数据中心的单个Host上只运行一个虚拟机，且虚拟机的资源配置和Host主机相同。本文不对Container迁移算法进行研究，在模拟器上设置容器禁止迁移，即不触发迁移阈值。由于只对容器应用调度研究，不执行具体的云任务，不设置容器、虚拟机和主机的PE数。测试负载是基于PlanetLab~\cite{Park2006CoMon}负载的改进，让CPU、内存、磁盘和带宽的利用率从10\%到90\%随机变化。虚拟机、主机的配置如表5.1所以，CPU的单位是Mips，容器应用根据负载情况随机构建。在云数据中心中，集群通常是异构的，节点拥有资源数量不同，假设存在三种主机，用于模拟异构集群，处理器MIPS的值通过查询处理器文档获得。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{主机和虚拟机配置表}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.5cm}<{\centering}|p{2cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|} \hline
		Host类型 & VM类型 & CPU型号 & MIPS & 内存(G) & 磁盘(G) & 带宽(M) \\ \hline
		\#1 & \#1 & i7 7500U & 49360 & 16 & 1000  & 100 \\ \hline
		\#2 & \#2 & i5 8200U & 65770 & 32 & 1000 & 100 \\ \hline
		\#3 & \#3 & X6 1100T & 78440 & 16 & 1000 & 100 \\ \hline
	\end{tabular}
\end{table}

容器应用的配置根据模拟的负载情况自动生成，并对生成的负载自动求解权重参数，参数设置部分代码片段如下：
\begin{lstlisting}[ language=Java]
public static final int VM_TYPES = 3;
public static final double[] VM_MIPS = new double[]{49360, 65770, 78440};
public static final int[] VM_PES = new int[]{};
public static final float[] VM_RAM = new float[] {(float)16000, (float) 32000, (float) 16000};//**MB*
public static final int VM_BW = 100;
public static final int VM_SIZE = 1000000;

public static final int HOST_TYPES = 3;
public static final int[] HOST_MIPS = new int[]{49360, 65770, 78440};
public static final int[] HOST_PES = new int[]{};
public static final int[] HOST_RAM = new int[]{1600,3200,1600};
public static final int HOST_BW = 100;
public static final int HOST_STORAGE = 1000000;

public static final int NUMBER_HOSTS = 30;
public static final int NUMBER_VMS = 30;
public static final int NUMBER_CLOUDLETS = 200;
\end{lstlisting}

模拟实验中设置资源阈值为0.85，节点上的某维度资源使用率超过该阈值后将不能部署新的容器应用，模拟实验主要测量在相同数量容器应用下各调度算法需要服务器的数量。若节点资源利用越均衡，该节点可部署的容器应用数量越多，从而需要的服务器数量越少。实验对比MRWS调度算法、Kubernetes的Default算法简称为KUB算法、Random算法以及FirstFit算法对资源的需求，后两种算法是ContainerCloudSim自带算法，KUB算法只需模拟内存和CPU均衡利用函数即可。MRWS是综合考虑CPU、内存、磁盘、网络带宽以及已部署Pod应用因素的综合评分算法。模拟实验中单位资源组$res=(cpu,memory,disk,bandwidth)=(10000Mips,4000M,100G,10M)$，单个应用容器的负载是单位资源组的$10\%\sim 90\%$随机变化的值，构建出的应用容器负载如表5.2所示。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{N个应用容器的资源配置}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		应用容器 & CPU(MIPS) & 内存(M) & 磁盘(G) & 带宽(M) & Pod \\ \hline
		 1 & 8500 & 680 & 34 & 9 &1 \\ \hline
		 2 & 4000 & 840 & 12 & 16 & 1 \\ \hline
		 3 & 2000 & 3400 & 20 & 6 & 1 \\ \hline
		 4 & 3400 & 1200 & 80 & 10 & 1 \\ \hline
		 ... & ... & ... & ... & ... & ... \\ \hline
		 N & 7600 & 600 & 30 & 4 & 1 \\ \hline
	\end{tabular}
\end{table}
使用FAHP自动建模并根据容器应用资源需求和单位资源组比值的差值构建模糊成对比矩阵，自动求解容器应用各维度资源的权重参数，上述容器列表各维度资源对应的权重参数如表5.3所示。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{应用容器资源对应的权重参数表}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		应用容器 & CPU(MIPS) & 内存(M) & 磁盘(G) & 带宽(M) & Pod \\ \hline
		 1 & 0.528 & 0.086 & 0.136 & 0.214 & 0.036 \\ \hline
		 2 & 0.185 & 0.128 & 0.082 & 0.570 & 0.035 \\ \hline
		 3 & 0.092 & 0.596 & 0.119 & 0.158 & 0.035 \\ \hline
		 4 & 0.140 & 0.095 & 0.508 & 0.221 & 0.036 \\ \hline
		 ... & ... & ... & ... & ... & ... \\ \hline
		 N & 0.596 & 0.092 & 0.158 & 0.119 & 0.035 \\ \hline
	\end{tabular}
\end{table}
测试在相同容器应用数量的条件下所需虚拟机的数量，主机Host和虚拟机配置相同，单个Host上只运行一个虚拟机，虚拟机的数量也是Host主机的数量。实验开始时给定一定数量的Host，三种类型Host数量相同，一旦出现所有主机都无法满足新的容器资源需求，同时增加三种类型Host各一台。如200个应用容器时先给定18台主机，以后每次各类型增加一台，即每次增加三台主机，避免某一类型的主机数量过多，测试结果如表5.4所示。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{相同容器数量下各算法所需主机数}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{算法}{容器数} & 200 & 400 & 600 & 800 & 1000 & 1200 \\ \hline
		MRWS & 24 & 45 & 63 & 81 & 96 & 111 \\ \hline
		KUB & 24 & 48 & 69 & 87 & 105 & 123 \\ \hline
		Random & 24 & 48 & 72 & 93 & 117 & 135 \\ \hline
		FirstFit & 27 & 54 & 81 & 108 & 138 & 159 \\ \hline
	\end{tabular}
\end{table}
从上表5.4可以得出，在应用容器数量和集群规模较小的情况下，各种算法对主机数量需求相差不大，MRWS调度算法的优势并不明显。这是由于集群中节点各维度资源过载现象较少，整体集群的资源利用率不也高。随着容器数量和主机数量的增加，Random和FirstFit算法劣势越加明显，相同数量下所需的主机数量更多。这是由于随机调度和首个合适节点调度方法容易造成大量相同密集型的应用部署到同一个节点，从而使某一维度资源过载，不能部署更多的应用。Kubernetes的Default算法考虑了CPU和内存的均衡性，其效果比其他两种算法要好，MRWS调度算法综合考虑了各维度资源的均衡性，其效果最佳，所需主机数量最少。MRWS还考虑节点已部署Pod因素，在资源利用率相差不大的情况下避免某个节点部署大量较小资源需求的容器应用，给节点容器管理造成过大开销。容器应用应该分散和均衡地部署到集群的各节点上，提升集群服务性能。

\subsection{MRWS负载均衡实验}
集群服务性能的关键在于集群的负载均衡性，而调度策略是影响负载均衡的核心因素。优秀的负载均衡策略不仅可以提升集群负载处理能力，缩短任务执行时间，也能有效避免单点因过载发生故障。当前服务器的负载均衡算法一般有随机算法、轮询和加权轮询、最小连接和加权最小连接、哈希算法、IP散列法以及URL散列等。一些常用的负载均衡组件有Apache、Nginx、LVS(Linux Virtual Server)、HAProxy、KeepAlived、Memcached等。Apache和Nginx是一个HTTP的服务器，具有反向代理能力，通过对用户请求分流实现负载均衡；LVS是一个虚拟的服务器集群系统，可以实现Linux平台下简单的负载均衡；HAProxy提供高可用、负载均衡以及基于TCP和HTTP的代理，适用于负载较大的web服务站点；KeepAlived也是一个组件，用于检测服务集群中故障节点，及时处理不健康的节点；Memcached是内存缓存系统，对于业务查询数据进行缓存，降低节点的负载，也是一个负载均衡组件。本文主要研究集群中各节点多维资源负载情况，目的在于研究新的调度方法是否具有更好的资源负载，各维度资源利用是否均衡。实验将从集群节点单维度资源利用率、节点各维度资源利用率均值、集群整体负载均衡性三个方面进行对比MRWS算法、Kubernetes的Default算法、Random和FirstFit算法的负载均衡度，负载均衡度衡量指标如式(3-10、3-11)所示。模拟实验在假定集群数量和应用容器数量相同的条件下进行，测量上述三个指标的负载不均衡度。

首先测试集群中各节点某个维度资源利用均衡情况，希望集群中各节点某个维度资源如CPU利用率趋于平滑，若节点上该维度资源波动过大则表明其负载不均，服务性能可能会降低，下面以CPU为例，其他维度资源类似。
\begin{figure}[h]
	\centering%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{firstfit-cpu}
		\caption{FirstFit算法CPU利用率}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{random-cpu}
		\centering{\caption{Random算法CPU利用率}}
	\end{subfigure}
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{kub-cpu}
		\caption{kubernetes算法CPU利用率}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{mrws-cpu}
		\centering{\caption{MRWS算法CPU利用率}}
	\end{subfigure}
	\caption{四种调度算法下节点CPU利用率波动图}
	
\end{figure}

从图5.4看出，MRWS调度算法下CPU利用率波动性较小，Kubernetes调度算法次之，Random和FirstFit算法节点CPU利用率波动较大。CPU波动性表明集群中节点CPU利用不均，有的节点CPU利用率过高，部分节点CPU利用率过低，集群的服务性能不稳定，高CPU利用率的节点容易出现资源过载。MRWS和Kubernetes算法都考虑了CPU利用的均衡性，其中Kubernetes在进行节点评分时考虑了CPU和内存平衡利用，MRWS调度算法综合考虑了CPU、内存、磁盘、网络带宽和已部署的Pod等因素的平衡性。有效避免单节点CPU维度资源利用过高，出现过载现象。一旦某个节点出现CPU利用过高，新的容器应用将不能调度到该节点上，造成其他维度的资源浪费，调度相同容器数量需要更多服务器。因此，MRWS算法能够有效提升集群的服务性能和资源利用率。

接下来的实验使用之前定义的均衡度对集群中单个节点上负载均衡度进行测量，实验中假设应用容器数量相同，分别用四种算法对其进行容器应用调度，计算单个节点的负载均衡度，部分节点的负载均衡度如下。

\begin{figure}[H]
	\centering%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{firstfit-ba}
		\caption{FirstFit算法节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{random-ba}
		\centering{\caption{Random算法节点负载均衡度}}
	\end{subfigure}
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{kub-ba}
		\caption{kubernetes节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{mrws-ba}
		\centering{\caption{MRWS算法节点负载均衡度}}
	\end{subfigure}
	\caption{四种调度算法下节点负载均衡度}
	
\end{figure}

如图5.5所示，四种调度算法中MRWS算法的负载均衡度最小，且波动也最小，Kubernetes算法次之，Random和FirstFit算法节点的负载均衡度波动最大并且节点负载均衡度较大。负载均衡度反映节点上各维资源空闲率的均衡程度，负载均衡度越大，各维资源消耗越不均衡，某个维度资源过载现象越严重。由于Random算法随机选择可用节点，FirstFit算法每次选择第一个满足资源需求的节点，完全不考虑各维资源的空闲情况，导致其负载均衡度较大，节点资源利用不充分。Kubernetes平衡CPU和内存利用率，其效果较另外两种要好，MRWS综合了各维度资源空闲率进行评分，其效果最佳，不仅负载均衡度整体偏小，其波动性也小。

下面进行四种调度算法下集群负载均衡度的对比，假定在容器应用数量不同的情况下，比较整个集群的负载均衡度情况。容器应用数量相同，集群负载均衡度越小，集群中过载的节点数量越少，相同情况下集群服务性能会越好，各维度资源消耗更加均匀，容器应用的调度策略更加合理。

\begin{figure}[H]
	\centering%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{firstfit-cluster}
		\caption{FirstFit算法节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{random-cluster}
		\centering{\caption{Random算法节点负载均衡度}}
	\end{subfigure}
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{kub-cluster}
		\caption{kubernetes节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{mrws-cluster}
		\centering{\caption{MRWS算法节点负载均衡度}}
	\end{subfigure}
	\caption{四种调度算法下集群负载均衡度}	
\end{figure}

如图5.6所示，MRWS调度算法下集群的负载均衡度最小，大概为Kubernetes算法的二分之一，Ramdom和FirstFit算法的四分之一。因此，在MRWS调度算法下集群中各节点相同维度资源消耗趋于平衡，单节点上各维度资源利用也较为均衡，出现资源过载现象的节点较少，集群的服务性能更优秀。集群的服务性能将多计算框架容器应用混合部署在Paladin容器云平台上进行测试，测试几种调度算方法下混合应用的执行时间。

\section{Paladin容器云平台研发部署}
由于不具备大规模实际容器云环境对几种调度算法进行测试，将在小规模容器云平台Paladin上实现集群性能对比，实验中使用多计算框架容器应用混合部署下同时进行数据处理来测试集群性能。因此，首先需要搭建一个Paladin实验环境，Paladin是自研的一个集海量数据存储管理，面向多计算框架资源调度，实现存储和计算分离的容器云平台。

\subsection{Paladin容器云平台介绍}
Paladin是一个支持用户构建多种大数据处理框架、并提供海量数据存储与管理的容器云平台。用户在平台上选择所需的计算与存储框架，如批处理(Hadoop、Spark)，流计算系统(Storm、Flink)、分布式的MPI、机器学习框架(Mahout、Tensorflow)、图计算系统（GridGraph、ReGraph)以及数据库系统(MongoDB、Mysql、Redis)等进行数据处理。平台能够快速构建相应的多计算框架运行环境、伸缩容器、并根据具体的需求提供计算任务调度、海量数据存储，使用优化的容器云多维资源利用率均衡调度算法提升集群资源利用率和服务性能。

\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{paladin-structure}
	\caption{Paladin平台架构图}
\end{figure}

从Paladin架构图可以看出，最底层是基础设施部分，即整个集群的物理机和云主机部分，中间是支持上层的中间软件服务如分布式文件系统、Docker容器服务、OpenShift Origin容器云等，第三层提供海量数据存储管理和多种数据处理计算框架，顶层是Web服务层。整个架构中最为重要的两个部分数是据存储管理模块Storage和数据处理框架模块Computing，数据存储管理模块是一个为用户提供数据上传、下载、删除、浏览、分享、磁盘挂载等分布式大数据管理存储系统。数据处理框架为用户提供快速构建各种计算框架容器集群环境，配置资源需求、增减容器数量以及应用权限管理等功能。该部分基于开源OpenShift Origin开发，在任何一个容器中都可以挂载用户存储的数据，实现存储与计算分离。当前Paladin容器云平台除支持单应用的Nginx、Apache、PHP、MongoDB等，还支持大量分布式数据处理框架如Hadoop、SPark、Storm、Spark、TensorFlow以及图计算项目Regraph、GridGraph等数十种计算框架运行环境的快速部署。用户既可以用该系统进行大数据存储管理，也能快速构建大数据处理框架，实现海量数据处理。系统管理员对基础设施、存储节点以及系统权限等进行管理。

\subsection{Paladin容器云平台部署}
首先需要构建小规模面向多计算框架的容器云平台Paladin，该平台集成了大数据存储和管理，实现了存储与计算分离功能。实验中采用6台物理机进行部署，一个主节点Matser，四个Node节点和一个存储S-Master节点，主节点同时也是Node节点。由于该存储需要对用户提供Web Service的服务，单独使用一个节点进行部署。将用户的存储管理服务和计算框架服务分离，分别部署在两个不同的物理机上，提升集群吞吐率，各物理节点的配置如下:
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{Paladin平台物理机配置}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{3cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{节点}{资源} & CPU核数 & 内存(G) & 磁盘(G) & 带宽(M) & IP \\ \hline
		Master & 4 & 16 & 1000 & 100 & 192.168.1.100  \\ \hline
		Node1 & 4 & 16 & 1000 & 100  & 192.168.1.101 \\ \hline
		Node2 & 4 & 16 & 1000 & 100  & 192.168.1.102 \\ \hline
		Node3 & 4 & 16 & 1000 & 100  & 192.168.1.103 \\ \hline
		Node4 & 4 & 16 & 1000 & 100  & 192.168.1.104 \\ \hline
		S-Matser & 4 & 8 & 500 & 100  & 192.168.1.111 \\ \hline
	\end{tabular}
\end{table}

各节点上需要安装的主要软件和组件图5.8所示，部署一个完整的大数据存储与处理容器云平台Paladin。在分布式计算框架Computing部分，Master节点上主要部署openshift-ansible及其依赖的各种软件包，用于安装对外提供大数据处理框架的openshift-origin服务集群、底层的Docker及其私有registry仓库、openshift-origin的键值存储系统Etcd、大数据存储管理需要的客户端paladin-client、经过优化改进的origin-web、主操作系统Centos 7.5。Master节点既作为容器云的主控节点，同时也是Node节点，在大数据存储系统中还作为数据存储节点。S-Master是大数据存储和管理的主控节点，使用Centos 6.9系统，安装Nginx服务、研发paladin-web-console对普通用户提供数据存储管理的web-console服务，同时也是数据存储节点。Node节点作为计算节点，安装Docker、openshift-origin集群子节点的需要的依赖组件如kubelet、HAProxy等、还有Etcd和paladin-client等。Node节点都是存储集群的数据节点，系统为Centos7.5。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{paladin-platform}
	\caption{Paladin平台节点分布图}
\end{figure}

安装完成后，Paladin平台的计算框架部分，查询集群活跃节点如下:
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=8cm]{paladin-nodelist}
	\caption{Paladin安装成功计算节点部分}
\end{figure}

paladin-web-console存储系统交互部分，用户可以进行存储管理，实现海量数据的上传、下载、删除、修改等操作。还能通过paladin-client将数据以磁盘的形式挂载到物理节点或者容器中，在分布式计算框架容器应用中以文件系统形式直接使用数据，实现存储和计算分离。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=7cm]{paladin-web-console}
	\caption{Paladin数据存储管理服务}
\end{figure}

整个Paladin平台部署完成后，在该平台上开发部署各种大数据处理框架和容器应用，平台origin-web-console用户服务界面如下:
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=7cm]{origin-web-console}
	\caption{Paladin数据处理框架部署服务}
\end{figure}

至此，整个Paladin大数据存储管理和计算框架快速部署容器云平台部署完成，实现了存储计算分离。用户既能进行大数据存储管理，实现普通的大数据云存储功能，又能快速构建大数据处理容器应用框架，实现海量数据处理，为普通用户提供优秀的大数据容器云配套服务。

\section{多计算框架容器应用开发}
在构建分布式计算应用时，容器数量动态变化需要处理框架及时发现并进行相应的调整，时刻保持计算环境可用。如在Hadoop容器应用框架下，有10个处理Pod，减少4个后，处理框架要能够快速发现并自动缩减为6个容器应用的Hadoop批处理环境。因此，分布式计算框架容器应用需要内嵌一个服务自发现组件如Zookeeper、Etcd、Serf等，处理框架用Serf作为服务注册和自发现组件。

\subsection{Serf服务发现}
在分布式计算框架中，有的是主从结构如Hadoop、Spark等，有的只是简单的计算框架如MPI等，并无主从之分，需要一个去中心化的服务解决方案。HashiCorp开源的Serf~\cite{Stubbs2015Distributed}是一个去中心化的服务发现和节点编排项目，具有成员管理、失败检测、错误恢复、高可用、分区容错等功能，是一个轻量级的服务发现组件。Serf底层实现Gossip协议，一种类似病毒感染的传播协议，能够快速自动感知节点上线和下线。构成分布式计算框架的所有容器应用上都维护一个成员列表，并将节点列表随机发送给周围节点，发送方式有Push和Pull两种，即主动发送和被动接受方式。Serf有一个时效检测器，定期轮询节点列表，给列表中的节点发送心跳包，剔除不在线的节点，确保所有的节点可用。

Serf在分布式处理框架的每个应用容器上运行一个Serf agent，在第一个容器应用上创建一个聚簇，后面创建容器的代理将会自动连接已存在的聚簇，形成一个集群。Serf是一个去中心化的服务方案，没有一个集中注册的节点，是一个弱一致性的方案，但具有较高的可用性和容错性。在常见的中心化集群中，集群的状态由Master维护，通常为了可靠性需要双备份甚至多备份主节点，主节点会成为整个集群的性能瓶颈。在集群规模较大时，信息的传播给网络带来巨大的负载，集群管理效率和正确性受到挑战，集群规模存在一个上限。去中心化的集群可以进行大规模扩展，其效率和可靠性几乎不受集群规模的影响，Kubernetes底层用Etcd做服务发现，其规模上限通常在一万左右，而Docker Swarm底层用Serf，其规模可以更大，Kubernetes的Etcd将会是制约其集群规模上限的重要因素。

Serf从功能上而言，自下而上可以分为三层：Gossip协议层、消息中间件、客户端接口。Gossip协议层封装Gossip协议以及集群管理的相关操作；消息中间件对传播消息进行封装，包括管理的指令、UserEvent、Query等各种处理逻辑，其中UserEvent是一个单向无需应答的消息，Query是需要应答的双向消息；客户端接口处理集群的输出输出格式，提供rpc接口。Serf是一个弱一致性的解决方案，提供一个简单、高效、可靠的去中心集群管理和服务发现系统实现。

\subsection{Hadoop处理框架容器应用构建}
在Paladin容器云平台计算框架部分，平台需要支持多种分布式处理框架，方便用户快速构建大数据处理环境。用户通过web-console与平台进行交互，只需使用鼠标操作就能构建所需的计算环境并配置容器资源，实现快速自动化部署。计算框架最终以模板Template存在集群中，所需的镜像文件存储在Docker私有仓库Registry中，所有用户都能重复使用该模板，其构建过程图5.12所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{hadoop-flow}
	\caption{Hadoop容器处理框架构建流程图}
\end{figure}

首先构建Hadoop容器应用的镜像文件，编写Hadoop Master和Slave的DockerFile文件，该文件在操作系统镜像ubuntu16.04的基础上安装Hadoop所需的环境，主要包括openjdk-8-jdk、hadoop-2.7.2、wget等。还需配置免密的dnsmasq、openssh-server以及服务自发现Hashicorp Serf，将Hadoop所需的配置文件进行修改，整体打包成镜像，部分DockerFile源码如下:
\begin{lstlisting}[ language=csh]
FROM ubuntu:16.04
MAINTAINER gongkunjxl <gongkunjxl@163.com>
RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list
RUN apt-get install -y unzip curl dnsmasq openssh-server
RUN apt-get install wget vim openjdk-7-jdk fuse libglib2.0-0
# dnsmasq configuration
ADD dnsmasq/* /etc/
# install serf
RUN curl -Lso serf.zip https://releases.hashicorp.com/serf/0.5.0/serf_0.5.0_linux_amd64.zip
RUN unzip serf.zip -d /bin 
RUN rm serf.zip
#ADD hadoop-2.7.2.tar.gz /root/
RUN wget XXX/hadoop-2.7.2.tar.gz(根据实际需要选择版本)
RUN tar -xzvf hadoop-2.7.2.tar.gz
RUN mv hadoop-2.7.2 /usr/local/hadoop
RUN rm hadoop-2.7.2.tar.gz
\end{lstlisting}

此外，还有一些修改后的配置文件和样例文件，以及挂载文件系统paladin-client的客户端等。使用docker build命令打包成Image文件，将文件push到私有仓库中，平台私有仓库的IP为172.30.7.23:5000，接下来的Template文件中将使用该镜像文件源。为重复使用该镜像文件以及用户通过web-console快速构建Hadoop计算框架，需要生成处理框架模板放置在平台上，其模板小部分代码如下:
\begin{lstlisting}[ language=csh]
{
"kind": "Template",
"apiVersion": "v1",
"metadata": {
"name": "hadoop-master",
"annotations": {
"description": "Hadoop Master",
"iconClass": "icon-hadoop",
"tags": "hadoop"
}
 "objects": [
{
"kind": "Service",
"apiVersion": "v1",
"metadata": {
"name": "${APP_SERVICE_NAME}",
},
.......
\end{lstlisting}

该模板包含一个Service、一个DeploymentConfig，部署完成后可以实现Container的自动伸缩。根据容器编排引擎Kubernetes底层网络原理，两个Pod之间可以实现IP直接互访，同一个Namespace下的Pod可以通过网桥直接通信。为了更好的实现对外服务，Pod的Service服务将其IP地址内嵌到后创建的同一Namespace下的所有Pod中，但在创建Service之前的Pod中不会嵌入IP，这是实现Pod构成集群的最好方式。利用网络管理的这一特性，使用Serf对第一个容器应用创建聚簇，后面容器的Serf代理通过第一个容器的内嵌IP实现集群发现服务。如第一个容器应用是hadoop-master，由于其创建了Service服务，后面创建的所有hadoop-slave容器中都会有\$HADOOP\_MASTER\_SERVICE\_HOST环境变量为hadoop-master的IP地址，以此实现集群的构建。通过web-console可以快速构建Hadoop处理环境，并根据资源需求实现容器的自动伸缩，按需配置服务。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=5cm]{hadoop-member}
	\caption{Hadoop集群成员}
\end{figure}

同样可以通过hadoop-master Pod的IP地址进行访问，在浏览器中访问Pod的50070和8088端口，查看Hadoop集群状态和运行任务:
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=5cm]{hadoop-status}
	\caption{Hadoop集群状态信息}
\end{figure}

至此，一个完整Hadoop容器应用处理框架构建完成，用户可以非常方便地在web-console上实现Hadoop环境的快速部署，并根据资源需求进行Pod数量动态伸缩，实现容器云平台的按需服务，并通过web查看器任务和集群状态。

\subsection{其他计算框架容器应用构建}
上一小节详细介绍了Paladin平台上Hadoop分布式处理框架的构建流程，类似上述的构建流程，构建多计算数据处理框架。原始的Docker hub中镜像文件大多只支持单个容器应如PHP、Go、Java、Ruby、Python等语言类容器应用，以及Mysql、MongoDB、MariaDB等数据存储类容器应用。分布式的数据处理框架和计算框架几乎没有，在Paladin上构建了离线数据批处理Hadoop、Spark、流计算Storm、Flink、分布式MPI、机器学习框架Mahout、Tensorflow、图计算系统GridGraph、Regraph等。众多的分布式处理框架可以满足绝大部分用户的需求，该平台将大数据存储管理和数据处理框架有机集合，并实现存储和计算分离。基于Docker技术构建的容器云平台整体性能比虚拟机式的云计算要优秀得多，容器技术减少了虚拟机系统隔离的开销。Paladin平台上构建的分布式应用模板如下:
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=7cm]{paladin-distribute}
	\caption{Paladin平台支持的处理框架模板}
\end{figure}

用户选择需要构建的处理框架，创建第一个容器应用及应用的Service，容器应用内部的Serf同时创建集簇，后创建的容器应用通过Service提供的IP用其Serf代理自动连接集簇，构建一个处理框架。根据实际的资源需求，用户动态调整Pod数量，伸缩其资源，实现按需服务。容器应用中的Serf代理会自动监控容器应用的伸缩，用户调整完Pod数量后无需重启或重新构建集群，Serf服务发现解决方案很好地解决了分布式框架容器应用自动伸缩问题。Paladin平台给用户提供一个web-console的可视化服务，用户无需了解部署原理和细节，甚至无需关心其构建过程，只需根据实际需求配置处理框架的容器数量，几秒内就可以搭建好开发测试环境。用户把所有的精力集中在应用的开发上，完全从集群和环境配置中解放出来。该平台同时还提供数据存储管理可视化，可以将数据挂载到所有的容器应用中，无需用户手动分配数据节点。

\section{多计算框架应用下MRWS性能测试}
部署完面向多计算框架的容器云平台Paladin后，需要对MRWS调度性能进行测试，并与Kubernetes、Random和FirstFit算法性能进行对比。由于是小规模的容器云集群，无法对资源利用率进行大规模测试，调度仿真在ContainerCloudSim已经完成，只进行多计算框架容器应用混合部署下集群性能的对比测试。

\subsection{混合部署多计算框架}
Paladin容器云平台只有5个实际的物理节点，在此小规模的平台上测试4种调度算法的性能，不进行大规模的调度测试。因此，构建几种较为典型的密集型应用容器，通过密集型应用任务完成时间测试4种调度算法的优劣。Hadoop作为离线数据批处理，在海量数据处理map阶段完成后不能将所有的数据都放入内存做shuffle，数据需要写入磁盘，其大部分应用都是I/O和网络密集型应用。用其数据读写样例作为I/O和网络密集型容器应用，即Hadoop框架作为I/O和网络密集型调度用例。Spark处理框架的特点是将大部分数据放入内存中运算，每次将map输出的结果保存在内存中，大部分应用是内存密集型应用。Spark的PageRank计算用作内存密集型调度用例。MPI多用于分布式计算，网络和计算资源通常是其瓶颈，MPI的PI计算是一种CPU密集型应用，可以用作CPU密集型调度用例。Paladin集群上将混合部署Hadoop、Spark和MPI三种计算处理框架，每个计算框架的容器数量相同，Hadoop上运行I/O和网络密集型应用TestDFSIO、Spark上运行内存密集型应用PageRank、MPI上运行CPU密集型应用PI计算。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{用于调度算法测试的计算框架}
	\begin{tabular}{|p{2.5cm}<{\centering}|p{2.5cm}<{\centering}|p{2.5cm}<{\centering}|p{2.5cm}<{\centering}|} \hline
		处理框架 & 类型 & 运行程序 & 容器数量 \\ \hline
		MPI & CPU密集 & PI计算 & 15 \\ \hline
		Spark & 内存密集 & PageRank & 15  \\ \hline
		Hadoop & I/O密集 & TestDFSIO & 15  \\ \hline
	\end{tabular}
\end{table}

Paladin平台的服务层是基于OpenShift Origin构建的，OpenShift Origin是一个开源的容器云平台，其底层的容器编排引擎是Kubernetes。该平台只是集成在Kubernetes上，并未修改其源码，因此，开发该平台的调度算法和Kubernetes平台上完全一致，最后只需要将调度算法的名称注册到平台的配置文件origin-master.conf中即可替代原有的调度方案。虽然Kubernetes提供了大量的API，但其使用较为繁琐，整个引擎的语言都是go开发，可以使用client-go进行快速开发。client-go是调用Kubernetes集群资源的客户端，对Kubernetes前置API的二次开发都可以使用client-go这个第三方包实现，能够对集群资源对象进行增删查改。调度算法都是使用client-go第三方包开发完成。

进行调度算法测试前，需要构建三种计算框架的容器应用，并对容器应用进行资源配置。根据其运行应用的类型，在相应维度上进行资源配置，如CPU密集型容器应用对CPU资源多配置。实验主要测试多计算框架混合部署下，几种调度算法对容器应用进行调度，集群完成全部调度任务的时间，用于衡量集群性能。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{Random调度算法应用容器分布}
	\begin{tabular}{|p{2cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{类型}{节点} & Master & Node1 & Node2 & Node3 & Node4 \\ \hline
		MPI-pod & 3 & 5 & 2 & 4 & 1 \\ \hline
		Spark-pod &3 & 2 & 3 & 2 & 5 \\ \hline
		Hadoop-pod & 1 & 4 & 5 & 2 &3 \\ \hline
		Total & 7 & 11 & 10 & 8 & 9 \\ \hline
	\end{tabular}
\end{table}
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{FirstFit调度算法应用容器分布}
	\begin{tabular}{|p{2cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{类型}{节点} & Master & Node1 & Node2 & Node3 & Node4 \\ \hline
		MPI-pod & 5 & 2 & 5 & 2 & 1 \\ \hline
		Spark-pod & 2 & 6 & 1 & 5 & 1 \\ \hline
		Hadoop-pod & 1 & 5 & 3 & 5 & 1 \\ \hline
		Total & 8 & 13 & 9 & 12 & 3 \\ \hline
	\end{tabular}
\end{table}
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{Kubernetes调度算法应用容器分布}
	\begin{tabular}{|p{2cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{类型}{节点} & Master & Node1 & Node2 & Node3 & Node4 \\ \hline
		MPI-pod & 3 & 3 & 3 & 3 & 3 \\ \hline
		Spark-pod &3 & 3 & 3 & 4 & 2 \\ \hline
		Hadoop-pod & 1 & 3 & 5 & 1 & 5 \\ \hline
		Total & 7 & 9 & 11 & 8 & 10 \\ \hline
	\end{tabular}
\end{table}
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{MRWS调度算法应用容器分布}
	\begin{tabular}{|p{2cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{类型}{节点} & Master & Node1 & Node2 & Node3 & Node4 \\ \hline
		MPI-pod & 3 & 4 & 3 & 3 & 2 \\ \hline
		Spark-pod &2 & 4 & 3 & 3 & 3 \\ \hline
		Hadoop-pod & 3 & 2 & 3 & 3 & 4 \\ \hline
		Total & 8 & 10 & 9 & 9 & 9 \\ \hline
	\end{tabular}
\end{table}
通过4种调度方案下应用容器的分布可以看出，Random随机调度三个计算框架的容器应用，不做任何资源空闲率负载均衡，CPU密集型容器应用MPI-pod在Node1上有5个，而Node4上只有1个。I/O密集型容器应用Hadoop-pod在Master上有1个，但Node2上有5个，各种容器应用随机调度分配，但是各节点上容器应用数量相差不大。这种调度方式必然会导致Node4上CPU资源紧缺，Node2上I/O资源紧缺，影响集群服务性能。FirstFit也不做资源使用率平衡，每次选择第一个满足资源的节点进行调度，导致前面的节点负载过高，后面节点负载较低，从整体容器应用分布来看，Node4上仅分配了三个容器应用，而Node1节点高达13个，导致部分节点维度资源负载都过高，部分节点资源利用率不足。Kubernetes对CPU和内存的资源空闲率做了平衡，尽量使内存和CPU资源平衡使用，从分布看出CPU密集型MPI-pod和内存密集型Spark-pod在各节点上的分布较为均匀，都在$2\sim4$个容器应用，大部分都是3个，但是I/O密集型应用Hadoop-pod分布不均，Node1和Node3上只有1个Hadoop-pod，Node2和Node4上有5个Hadoop-pod，这种只注重CPU和内存均衡的调度方式导致集群各节点I/O密集型分布不均，影响集群性能。MRWS调度方式可以看到几种密集型应用分布较为均衡，各节点的各种密集型容器应用都在$2\sim4$个，单个节点容器数量也在$8\sim10$个之间。整个集群各维度资源利用率较为均衡，集群负载均衡性较好。

\subsection{单个计算框架服务性能}
针对四种调度方案下混合部署多计算框架容器应用，首先进行单个计算框架的性能测试，在容器应用中设置资源的下限，不限制应用容器使用资源的量，同一个节点上的容器应用可以对节点资源进行自由竞争。对比四种调度算法下单个计算框架应用执行的时间，在Hadoop-pod中运行TestDFSIO程序，分别读写15个128M的文件，记录读写总时间。Spark-pod中运行PageRank程序，测试文件大小约3.2M，轮数为2000轮迭代。MPI-pod中运行PI计算程序，15个节点进行110000轮迭代。每个调度算法分别运行三次，取三次运算时间的平均值，各计算框架容器应用单独执行时间如下(单位:秒)。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{四种调度算法应用容器执行时间}
	\begin{tabular}{|p{3cm}<{\centering}|p{3.5cm}<{\centering}|p{3cm}<{\centering}|p{2.5cm}<{\centering}|} \hline
		\diagbox[innerwidth=3cm]{调度}{容器应用} & Hadoop-TestDFSIO & Spark-PageRank & MPI-PI \\ \hline
		Random & 255.92 & 267.33 & 387.19  \\ \hline
		FirstFit & 276.48 & 274.67 & 390.69  \\ \hline
		Kubernetes & 250.44 & 252.00 & 214.42  \\ \hline
		MRWS & 228.33 & 250.52 & 212.88  \\ \hline
	\end{tabular}
\end{table}
对比四种调度算法单计算框架下容器应用的执行时间，可以很明显地发现进行I/O密集型容器应用Hadoop-TestDFIO测试时，Random、FirstFit以及Kubernetes调度算法没有考虑I/O资源均衡利用，导致多个I/O密集型应用被调度到同一节点。在容器中运行实际应用时，大量I/O密集型应用竞争I/O和网络资源，导致应用效率低下，执行时间过长。MRWS调度算法考虑I/O和网络带宽因素，尽量分散到集群各节点上，其性能最佳，Hadoop-TestDFSIO执行时间最短，FirstFit效果最差，Random和Kubernetes相差不大。在单独运行内存密集型应用Spark-PageRank容器应用下，Random和FirstFit都不考虑内存利用率均衡性，其运行时间最长，尤其是FirstFit将大量内存密集型调度到了同一个节点，使得内存成为该节点瓶颈，运行时间最长。Random随机分配节点，其效果一般，Kubernetes和MRWS均做了内存利用率均衡，其效果较好，各内存密集型容器大致可以分散到集群各节点上。与内存类似，CPU密集型应用表现最为明显，PI运算是一个典型的计算密集型应用，大量的时间花费在三角函数的运算上，Random和FirstFit均不做CPU利用率均衡，导致大量的CPU密集型应用调度到同一个节点，该节点的CPU成为其性能瓶颈，运行时间较长。Kubernetes和MRWS调度可以将CPU密集型应用尽可能分散到各节点上，对CPU空闲率进行均衡，使得任务运行时间大大缩短。

\subsection{混合部署多计算框架下的服务性能}
混合部署三种密集型容器应用框架，同时运行Hadoop-TestDFSIO、Spark-PageRank和MPI-PI容器应用程序，对比几种调度算法的执行效率。混合部署和同时运行不同计算框架类似于多用户在容器云集群上部署多种计算框架容器应用的场景，不仅相同密集型容器应用需要对节点资源进行竞争，其他计算框架也会参与资源竞争。比如同一节点上MPI-pod对CPU进行竞争，该节点上部署的Spark-pod也会参与竞争CPU资源，调度方案不合理将导致整个集群性能低下，所有用户应用执行效率都将受到影响。每种调度算法执行3次，取3次执行时间的平均值，四种调度下同时运行多计算框架容器应用执行时间如下(单位:秒)。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{同时执行多计算框架应用容器执行时间}
	\begin{tabular}{|p{3cm}<{\centering}|p{3.5cm}<{\centering}|p{3cm}<{\centering}|p{2.5cm}<{\centering}|} \hline
		\diagbox[innerwidth=3cm]{调度}{容器应用} & Hadoop-TestDFSIO & Spark-PageRank & MPI-PI \\ \hline
		Random & 316.46 & 584.00 & 418.43  \\ \hline
		FirstFit & 355.81 & 603.33 & 454.71  \\ \hline
		Kubernetes & 299.87 & 503.33 & 271.33  \\ \hline
		MRWS & 269.10 & 491.66 & 256.62  \\ \hline
	\end{tabular}
\end{table}
首先将执行时间和单个计算框架容器应用单独执行时间进行纵向对比，在混合部署同时执行多计算框架下所有应用执行时间都变长。如Hadoop-TestDFSIO在MRWS调度下单独执行时间是228.33，在混合部署执行下269.10，这是由于更多的容器应用参与资源竞争导致整体执行时间变慢。Spark-PageRank执行时间变化最为明显，因为PageRank的运算不仅需要大量的内存资源，还需要一定的CPU，而CPU资源被MPI-PI计算框架占用，导致其执行时间变慢。

横向对比各调度算法混合部署多计算框架下同时运行容器应用执行时间，FirstFit任务处理时间最长。这是由于其每次选择第一个可用节点导致其资源利用不均，部分节点资源利用不足。Random和Kubernetes相较于MRWS在Hadoop-pod的性能较差，MRWS调度算法性能最优，单个任务的执行时间相较于其他调度算法执行都较短。因为MRWS调度算法综合考虑了容器应用资源需求以及节点的CPU、内存、磁盘、网络带宽和节点已部署Pod等因素，利用FAHP自动建模和求解容器应用多维资源权重参数，利用权重参数对节点综合评分，选取最优的节点作为容器应用调度目标。这种调度方式在多计算框架下性能表现最为突出，通常在用计算框架进行数据处理时Hadoop需要大量I/O和网络带宽，Spark需要大量内存资源，MPI需要计算资源较多，混合部署下需要平衡各维度资源的利用率。各计算框架处理任务不同，资源需求也不尽相同，在混合部署时需要充分考虑各计算框架的特点才能充分利用容器集群资源，缩短多计算框架容器应用在容器云平台上的任务处理时间，提升集群服务性能。

\section{本章小结}
本章从实验的角度对比四种调度算法性能优劣，首先详细介绍云计算仿真平台CloudSim和在此基础上研发的容器云仿真平台ContainerCloudSim。在容器云仿真平台上对比几种调度方法的集群资源利用率和负载均衡度，MRWS相较于其他几种调度方法具有更低的负载均衡度和更高的资源利用率。接着介绍和部署面向多计算框架的容器云平台Paladin，该平台是一个存储与计算分离的容器云平台。然后在该平台上构建数十种大数据处理框架，并以Hadoop批处理框架为例详细介绍其构建流程，用户在几秒内实现大数据处理框架快速构建并进行容器伸缩扩展。最后，在Paladin平台上进行多计算框架容器应用调度分析，对比几种调度算法任务处理性能。分别以I/O和网络密集型应用Hadoop-TestDFSIO、内存密集型应用Spark-PageRank和CPU密集型应用MPI-PI计算框架容器应用为调度用例，在混合部署几个计算框架时单独和同时进行任务处理，对比各计算框架容器应用的执行时间，几种算法中MRWS的任务处理时间更短。



















