\chapter{OpenShift容器云平台和MRWS实验}
提出MRWS调度方案优化OpenShift容器云平台底层容器编排引擎Kubernetes调度流程和调度算法后，需要对其资源利用率和负载均衡性进行评估测试。本章主要实验包括如下几个部分:
\begin{enumerate}[(1)]
	\item 在ContainerCloudSim容器云仿真平台上进行大规模容器应用仿真调度实验、对比Kubernetes默认算法、Random算法和MRWS调度算法在资源利用率和负载均衡性方面的性能。
	\item 基于开源OpenShift Origin搭建开发了Paladin容器云平台，该平台是一个集海量数据存储管理、多计算框架快速部署、调度优化、用户注册、按需服务等多功能的容器云PaaS平台。
	\item 在Paladin开发多种分布式计算框架，将其打包成镜像文件，存储在镜像仓库汇中，用户可以快速构建开发测试环境。
	\item 在Paladin平台上开发MRWS算法的调度器，使用多中计算框架容器应用混合部署测试其性能，并与其他调度算法进行对比。
\end{enumerate}

\section{ContainerCloudSim模拟MRWS调度方案}
实验室基于开源OpenShift Origin开发的容器云PaaS平台是在小规模实际物理机上部署，不能进行调度方案的大规模测试和分析，为了评估方案的性能和可靠性，首先在容器云仿真平台ContainerCloudSim上进行大规模的仿真实验。首先构建仿真实验环境，然后开发Kubernetes默认Default调度算法、Random和MRWS调度算法，使用大规模负载进行测试分析。

\subsection{ContainerCloudSim容器云仿真平台}
\subsubsection{CloudSim云仿真平台}
容器和容器编排技术的逐渐成熟推动了容器云的飞速发展，在计算中心的容器云上为了评测资源调度策略和容器服务性能，一个容器云仿真平台变得异常重要。经过大量测试对比调度方案，不仅可以节约开发时间，也能避免造成资源浪费和减少试错成本。针对每一个调度方案，如果部署大规模容器云平台进行性能分析测试，绝大多数的小公司和开发人员并不具备这种条件。在传统的云计算模式下，应用服务的组成、供应、配置和部署条件较为复杂，当用户需求和系统配置动态变化时，评估一个调度策略以及工作负载是相当困难的，一个优秀的云计算平台模拟器可以很好解决这个问题。一个云平台模拟器通过控制环境变量和重复试验可以加速理论研究和开发过程，根据需求和应用场景不同，各大公司和研究机构推出了一系列的云计算平台仿真工具。

MDCSim是一个全面、灵活、可扩展的多层数据中心模拟器，整个模拟平台分为通信层、内核层和用户层建模，通信层用于模拟模拟集群内部通信、内核层模拟调度和分析系统性能、用户层用于模拟各种应用。该模拟器可以根据底层不同硬件特点进行混合建模，用于评估数据中心的能耗，让用户在保持低功耗的同时实现集群服务性能的提升。GroudSim是一个基于Java的模拟器，用于模拟科学应用在网格和云设施上执行问题，模拟完成后给用户提供了基础的统计和分析功能。NetworkCloudSim用于解决网络模拟问题，弥补其他模拟器对网络细节关注不足，支持MPI和工作流。TeachCloud用于对MapReduce应用建模并集成一个负载生成器，提供图形化的接口和实现定制的网络拓扑结构。CloudSim是墨尔本大学Gridbus项目推出的云计算仿真软件，既能对系统性能和应用服务模拟、仿真、试验，也能评估资源调度策略的优劣。

CloudSim是一个开源的仿真软件，最大特点就是提供一个虚拟化引擎，帮助数据中心建立和管理各种虚拟化服务。支持大规模云计算资源管理和调度模拟，将数据中心的资源虚拟化为资源池，CloudSim的分层体系架构图如5.1所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{cloudsim-layer}
	\caption{CloudSim分层架构图}
	\label{fig:xfig1}
\end{figure}

CloudSim是一个分层构建的体系结构，从下到上一次为CloudSim核心引擎层、网络、云资源、云服务、虚拟服务、用户接口结构、资源调度以及仿真规格层。下面一次介绍各层次的大致作用:
\begin{enumerate}[(1).]
	\item CloudSim核心引擎层是离散数据模拟引擎SimJava，为上层提供系统组件构建如服务、数据中心、客户端、代理、虚拟机等，查询和时间处理、通信、模拟时钟等功能。
	\item Network层模拟网络组件如资源收集、数据集、负载测试、信息服务等，对网络设施建模，支持高层软件组件。
	\item 云资源层主要是Host主机和数据中心，主机的核心硬件设施通过数据中心类建模，处理服务请求，构成虚拟资源池。
	\item 云服务层给客户端分配特定应用的VM，同时给VM分配处理内核、内存、磁盘以及网络带宽，可以执行用户新的VM提供策略，有助于一定目标优化。
	\item 虚拟机服务层提供任务执行和虚拟机管理，定义一系列虚拟机创建、销毁、合并、迁移等操作管理，执行基于云环境的应用服务。
	\item 用户接口层向用户提供VM任务单元和虚拟机，将下层的虚拟资源打包成虚拟机提供给用户。
	\item 用户代码层是用户根据实际应用场景和需求，定制应用的规格和调度策略，将应用加入数据中心的代理中，按照资源调度策略进行调度。
\end{enumerate}
CloudSim有一些重要的类和核心概念，针对这些类的大致作用进行简单的介绍:
\begin{enumerate}[1.]
	\item DataCenter类封装底层的Host主机，提供虚拟化的资源，保证每个数据中心至少存在一台运行的Host主机，同时提供虚拟化网络并内置了一个调度组件，为虚拟机和主机分配CPU、内存、网络带宽等资源。
	\item DataCenterBroker类是数据中心代理，负责虚拟机和云任务列表的提交。
	\item VM类是虚拟机类，运行在Host上，多个VM共享Host资源。
	\item Cloudlet类是云任务类，根据用户的设置构建云计算和调度任务。
	\item VmAllocationPolicy类是虚拟机分配策略类，该类实现了虚拟机分配给Host主机的调度策略，用户可以重写该分配策略。
	\item CloudletScheduler类实现多种分配策略，虚拟机内部应用共享处理器的策略，时间共享还是空间共享。
\end{enumerate}

此外，CLoudSim还有数据中心资源配置类DataCenterCharacteristics、扩展虚拟机分配策略的主机类Host、带宽分配策略类BwProvisioner、模拟网络延时行为类NetworkTopology、模拟存储区域网类SanStorage、虚拟分配主存类RamProvisioner、云协调器类CloudCoordinator等一些列重要的类，共同完成CloudSim功能。

\subsubsection{ContaienrCloudSim容器云仿真平台}
随着容器技术的迅猛发展，CaaS(Congtianer as a Service)作为一种新型的服务模式变得越来越普遍。上述介绍的各种云计算仿真平台以及早期的CloudSim版本并不支持容器仿真，一个支持容器应用仿真的平台变得越加紧迫。为了缩短容器云上新方法的开发时间，墨尔本大学的研究人员利用CloudSim虚拟化的特点，在其基础上开发了ContainerCloudSim，专门用于数据中心容器应用的模拟。在最新的CloudSim-4.0上已经集成ContainerCloudSim，提供Docker容器应用仿真支持，ContainerCloudSim与CloudSim生态圈关系如图5.2所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{container-cloudsim}
	\caption{ContainerCloudSim与CloudSim生态圈}
\end{figure}
集成ContainerCloudSim的CloudSim-4.0云平台模拟器已完整支持容器云的仿真，新的版本具有如下几个特点:
\begin{enumerate}[(1)]
	\item 支持数据中心大规模云计算建模和仿真。
	\item 支持服务器虚拟化和主机的建模仿真，定制虚拟机调度策略。
	\item 支持容器应用程序的建模和仿真。
	\item 支持能量感知的计算资源建模和仿真。
	\item 支持网络拓扑结构和消息传递应用建模和仿真。
	\item 支持动态插入元素、停止和恢复的模拟。
	\item 支持混合云的建模和仿真。
\end{enumerate}

在ContainerCloudSim部分，提供容器、VMs、Host、数据中心资源包括CPU、内存和存储的管理功能，实现动态监控系统性、控制容器内应用的执行以及给容器提供虚拟机。容器的模拟器要能给研究人员提供容器调度方案间的对比，容器调度策略决定容器如何被调度到虚拟机上，以及各种调度算法之间的对比和评测。算法的能耗问题也是容器模拟器应该关注的重点，可以提供各种算法的能耗度量，容器的合并和迁移也是模拟器的一大功能。最后，模拟器要能够支持容器的扩展性，在CaaS容器环境中，容器的数量是远远多于虚拟机的。ContainerCloudSim是在CloudSim基础上开发而来，也是一个分层的架构，整体架构如下所示。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{containercloudsim-layer}
	\caption{ContainerCloudSim分层架构图}
\end{figure}
从底至上依次分为数据中心管理服务、能耗监控服务、资源分配服务、资源管理服务、虚拟机生命周期管理服务、容器生命周期管理服务和负载管理服务等。每个层次的大致作用如下:
\begin{enumerate}
	\item 负载管理服务层关注于客户端应用的注册、部署、调度、应用层级的性能以及应用健康监控。
	\item 容器生命周期服务管理层负责容器生命周期管理，包括创建容器、注册容器到系统中、启动、停止、重启、从一个主机迁移到另一个主机以及容器销毁。除此之外，还负责执行和管理在容器中任务，监控任务资源利用率。
	\item 虚拟机生命周期管理服务层负责虚拟机的管理包括创建、启动、停止、重启、销毁、迁移以及资源利用率监控。
	\item 资源管理服务层负责容器在满足资源需求和软件环境的虚拟机上创建，虚拟机在满足资源需求的主机上创建，由容器调度、虚拟机调度和合并服务构成。容器调度根据容器调度策略调度容器到虚拟机、虚拟机调度根据虚拟机调度策略调度虚拟机到主机，合并策略通过合并容器减少主机需求，最小化资源碎片。
	\item 资源分配层服务管理虚拟机和容器的资源分配，由容器分配服务和虚拟机分配服务构成。容器分配服务负责虚拟机资源分配给容器，虚拟机分配服务负责主机资源分配给虚拟机。
	\item 能耗监控服务负责数据中心主机能耗监控，构建必要的能耗模型。
	\item 数据中心管理服务负责管理数据中心资源，主机开关机以及监控资源利用率。
\end{enumerate}
至此，ContainerCloudSim容器云仿真平台介绍完毕，具体的仿真执行流程具体的实现可以参见其源代码。

\subsection{MRWS资源利用率实验}
在ContainerCloudSim容器云仿真平台上的ContainerPlacementPolicy库下开发MRWS、Kubernetes默认Default算法，该类自带Ramdom算法，对比三种算法下集群资源的利用率。该容器云仿真平台影响调度因素较多，需要根据实际的需要进行一些必要的设置。

首先MRWS算法和Kubernetes的Default算法在预选阶段使用的筛选规则相同，评分阶段除空闲资源评分函数和平衡函数外，其他评分函数相同。因此，假设其他评分函数和外部条件都相同的情况下，只需模拟比较MRWS算法和Default算发的空闲资源评分函数。在模拟器设置方面，要对比容器各种调度算法下集群资源的利用率和负载均衡性，对虚拟机的调度策略已有众多的研究，不是本文的研究方向，因此，假设ContainerCloudSim数据中心的单个Host上只运行一个虚拟机，且虚拟机的资源配置和Host主机相同。本文也不对Container的迁移算法做研究，需要在模拟器上设置容器禁止迁移，即不触发迁移阈值。由于只对容器应用调度研究，不执行具体的云任务，不设置容器、虚拟机和主机的PE数。测试负载是基于PlanetLab负载进行的更改，让CPU、内存、磁盘和带宽的利用率从10\%到90\%的随机变化。虚拟机、主机的配置如表5.1所以，CPU的单位是Mips，容器应用根据负载情况随机构建。在云数据中心中，集群通常是异构的，节点拥有资源数量不同，假设存在三种主机，用于模拟异构集群。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{主机和虚拟机配置表}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.5cm}<{\centering}|p{2cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|} \hline
		Host类型 & VM类型 & CPU型号 & MIPS & 内存(G) & 磁盘(G) & 带宽(M) \\ \hline
		\#1 & \#1 & i7 7500U & 49360 & 16 & 1000  & 100 \\ \hline
		\#2 & \#2 & i5 8200U & 65770 & 32 & 1000 & 100 \\ \hline
		\#3 & \#3 & X6 1100T & 78440 & 16 & 1000 & 100 \\ \hline
	\end{tabular}
\end{table}

容器应用的配置根据模拟的负载情况自动生成，并对生成的负载自动求解权重参数，参数设置部分代码片段如下：
\begin{lstlisting}[ language=Java]
public static final int VM_TYPES = 3;
public static final double[] VM_MIPS = new double[]{49360, 65770, 78440};
public static final int[] VM_PES = new int[]{};
public static final float[] VM_RAM = new float[] {(float)16000, (float) 32000, (float) 16000};//**MB*
public static final int VM_BW = 100;
public static final int VM_SIZE = 1000000;

public static final int HOST_TYPES = 3;
public static final int[] HOST_MIPS = new int[]{49360, 65770, 78440};
public static final int[] HOST_PES = new int[]{};
public static final int[] HOST_RAM = new int[]{1600,3200,1600};
public static final int HOST_BW = 100;
public static final int HOST_STORAGE = 1000000;

public static final int NUMBER_HOSTS = 30;
public static final int NUMBER_VMS = 30;
public static final int NUMBER_CLOUDLETS = 200;
\end{lstlisting}

模拟实验中设置资源阈值为0.15，一旦节点上的某种资源使用率超过该阈值后将不能部署新的容器应用，模拟实验主要测量在相同数量容器下需要服务器的数量。若节点资源利用越均衡，该节点可部署的容器应用数量越多，从而需要的节点数量越少。实验对比MRWS调度算法、Kubernetes的Default算法简称为KUB算法、Random算法以及FirstFit算法对资源的需求，后两种算法是ContainerCloudSim自带算法，只需要模拟KUB的内存和CPU均衡利用，MRWS综合考虑CPU、内存、磁盘、网络带宽以及已部署Pod应用因素的综合评分算法。模拟实验中单位资源组$res=(cpu,memory,disk,bandwidth)=(10000Mips,4000M,100G,10M)$，单个应用容器的负载是单位资源组的$10\%\sim 90\%$随机变化的资源需求，构建出的应用容器负载如表5.2所示。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{N个应用容器的资源配置}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		应用容器 & CPU(MIPS) & 内存(M) & 磁盘(G) & 带宽(M) & Pod \\ \hline
		 1 & 8500 & 680 & 34 & 9 &1 \\ \hline
		 2 & 4000 & 840 & 12 & 16 & 1 \\ \hline
		 3 & 2000 & 3400 & 20 & 6 & 1 \\ \hline
		 4 & 3400 & 1200 & 80 & 10 & 1 \\ \hline
		 ... & ... & ... & ... & ... & ... \\ \hline
		 N & 7600 & 600 & 30 & 4 & 1 \\ \hline
	\end{tabular}
\end{table}
根据MRWS的FAHP自动建模和按照资源需求比值的差值构建模糊成对比矩阵，自动求解容器应用各维度资源的权重参数，上述容器列表各维度资源对应的权重参数如表5.3所示。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{应用容器资源对应的权重参数表}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|p{1.8cm}<{\centering}|} \hline
		应用容器 & CPU(MIPS) & 内存(M) & 磁盘(G) & 带宽(M) & Pod \\ \hline
		 1 & 0.528 & 0.086 & 0.136 & 0.214 & 0.036 \\ \hline
		 2 & 0.185 & 0.128 & 0.082 & 0.570 & 0.035 \\ \hline
		 3 & 0.092 & 0.596 & 0.119 & 0.158 & 0.035 \\ \hline
		 4 & 0.140 & 0.095 & 0.508 & 0.221 & 0.036 \\ \hline
		 ... & ... & ... & ... & ... & ... \\ \hline
		 N & 0.596 & 0.092 & 0.158 & 0.119 & 0.035 \\ \hline
	\end{tabular}
\end{table}
测试在相同容器应用数量的条件下所需虚拟机的数量，根据前面一个主机Host上只运行一个和主机配置相同虚拟机，因此虚拟机的数量也是Host主机的数量。实验开始时给定一定数量的Host，三种类型Host数量相同，一旦出现所有主机都无法满足新的容器资源需求，同时增加三种类型Host一台。如200个应用容器时先给定18台主机，以后每次各类型增加一台，即每次增加三台主机，避免某一类型的主机数量过多，测试结果如表5.4所示。
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{相同容器数量下各算法所需主机数}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{算法}{容器数} & 200 & 400 & 600 & 800 & 1000 & 1200 \\ \hline
		MRWS & 21 & 42 & 60 & 78 & 96 & 114 \\ \hline
		KUB & 21 & 45 & 69 & 87 & 105 & 129 \\ \hline
		Random & 24 & 48 & 72 & 93 & 117 & 139 \\ \hline
		FirstFit & 27 & 54 & 81 & 108 & 138 & 162 \\ \hline
	\end{tabular}
\end{table}
从上表5.4可以得出，在应用容器数量集群规模较小的情况下，各种算法对主机数量需求相差不大，MRWS调度算法的优势并不明显。这是由于集群中节点各维度资源过载现象较少，整体集群的资源利用率不也高。随着容器数量和主机数量的增加，Random和FirstFit算法劣势越加明显，相同数量下所需的主机数量更多，这是由于随机调度和首个合适节点调度方法容易造成大量相同密集型的应用部署到同一个节点，从而使某一维度资源过载，不能部署更多的应用。Kubernetes的Default算法考虑了CPU和内存的均衡性，其效果比其他两种算法要好，MRWS调度算法综合考虑了各维度资源的均衡性，其效果最佳，所需主机数量最少。MRWS还考虑节点已部署Pod因素，在资源利用率相差不大的情况下避免某个节点部署大量较小资源需求的容器应用，给节点容器管理造成过大开销，从而降低集群性能。容器应用应该分散和均衡地部署到集群的各节点上，提升集群服务性能。

\subsection{MRWS负载均衡实验}
集群服务性能一个关键问题就是负载均衡，而调度策略是影响负载均衡的核心因素。好的负载均衡策略不仅可以提升集群负载处理能力，缩短任务执行时间，也能有效避免单点故障。当前服务器的负载均衡算法一般有随机算法、轮询和加权轮询、最小连接和加权最小连接、哈希算法、IP散列法以及URL散列等。一些重用的负载均衡组件有Apache、Nginx、LVS(Linux Virtual Server)、HAproxy、KeepAlived、Memcached等。Apache和Nginx是一个HTTP的服务器，具有反向代理能力，通过对用户请求分流实现负载均衡；LVS是一个虚拟的服务器集群系统，可以实现Linux平台下简单的负载均衡；HAproxy提供高可用、负载均衡以及基于TCP和HTTP代理，适用于负载较大的web服务站点；KeepAlived也是一个组件，用于检测服务集群中故障节点，及时处理不健康的节点；Memcached是内存缓存系统，对于业务查询数据进行缓存，降低节点的负载，也是一个负载均衡组件。本文主要研究集群中各节点多维资源负载情况，目的在于新的调度方法是否具有更好的资源负载，各维度资源利用是否更加均衡。实验将从三个方面进行对比MRWS算法、Kubernetes的Default算法、Random算法和FirstFit算法中单个节点的相同维度资源、单个节点各维度资源均值、整个集群负载均衡性进行对比，评价指标如式(3-10、3-11)所示。测试过程中假定集群数量、应用容器数量不同的条件下上述三个指标的负载不均衡度。

首先测试集群中各节点某个维度资源利用均衡情况，希望集群中节点上某个维度资源如CPU利用率更加平滑，若节点上该维度资源波动过大则表明其服务性能可能会降低，下面以CPU为例，其他维度资源类似。
\begin{figure}[h]
	\centering%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{firstfit-cpu}
		\caption{FirstFit算法CPU利用率}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{random-cpu}
		\centering{\caption{Random算法CPU利用率}}
	\end{subfigure}
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{kub-cpu}
		\caption{kubernetes算法CPU利用率}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{mrws-cpu}
		\centering{\caption{MRWS算法CPU利用率}}
	\end{subfigure}
	\caption{四种调度算法下节点CPU利用率波动图}
	
\end{figure}

从图5.4看出，MRWS调度算法下CPU利用率波动性较小，Kubernetes调度算法次之，Random和FirstFit算法节点CPU利用率波动较大。这是由于前两种算法都考虑了CPU利用的均衡性，其中Kubernetes在进行节点评分时考虑了CPU和内存平衡利用，MRWS调度算法综合考虑了CPU、内存、磁盘、网络带宽和已部署的Pod的因素。有效避免单个节点某个维度资源资源利用过高，出现过载现象，从而不能调度更多的应用，造成其他维度的资源浪费，MRWS算法能够有效提升集群的服务性能和资源利用率。下面的实验使用之前定义的均衡度对集群中单个节点上资源均衡度进行测量，实验中假设应用容器数量相同，分别用四种算法对其进行调度，计算单个节点的负载均衡度，部分节点的负载均衡度如下。

\begin{figure}[H]
	\centering%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{firstfit-ba}
		\caption{FirstFit算法节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{random-ba}
		\centering{\caption{Random算法节点负载均衡度}}
	\end{subfigure}
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{kub-ba}
		\caption{kubernetes节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{mrws-ba}
		\centering{\caption{MRWS算法节点负载均衡度}}
	\end{subfigure}
	\caption{四种调度算法下节点负载均衡度}
	
\end{figure}

如图5.5所示，四种调度算法中MRWS算法的负载均衡度最小，且波动也最小，Kubernetes算法次之，Random和FirstFit算法节点的负载均衡度波动最大并且节点负载均衡度较大。负载均衡度反映节点上各位资源空闲率的均衡程度，负载均衡度越大，各位资源消耗越不均衡，某个维度资源过载现象越严重。由于Random算法随机选择可用节点，FirstFit算法每次选择第一个满足资源需求的节点，完全不考虑各维资源的空闲情况，导致其负载均衡度较大，节点资源利用不充分。Kubernetes平衡CPU和内存利用率，其效果较另外两种要好，MRWS综合了各维度资源空闲率进行评分，其效果最佳，不仅负载均衡度整体偏小，其波动性也小。

接下来进行四种调度算法下集群负载均衡度的对比，假定在容器应用数量不同的情况下，比较整个集群的负载均衡度情况。应用数量相同，负载均衡度越小，集群中过载的节点数量越少，相同情况下集群服务性能会越好，各维度资源消耗更加均匀，应用的调度更合理。

\begin{figure}[H]
	\centering%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{firstfit-cluster}
		\caption{FirstFit算法节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{random-cluster}
		\centering{\caption{Random算法节点负载均衡度}}
	\end{subfigure}
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm,height=5cm]{kub-cluster}
		\caption{kubernetes节点负载均衡度}
	\end{subfigure}%
	\hspace{0.5cm}%
	\begin{subfigure}{7cm}
		\includegraphics[width=7cm, height=5cm]{mrws-cluster}
		\centering{\caption{MRWS算法节点负载均衡度}}
	\end{subfigure}
	\caption{四种调度算法下集群负载均衡度}	
\end{figure}

如图5.6所示，MRWS调度算法集群的负载均衡度最小，大概为Kubernetes算法的二分之一，Ramdom和FirstFit算法的四分之一。因此，在MRWS调度算法的集群中各节点资源消耗情况趋于均衡，单个节点上各维度资源利用也较为均衡，集群的服务性能更优秀。针对集群的服务性能将使用多计算框架应用混合部署在实际的实验平台上进行测试，测试集中调度算方法下混合应用的完成时间。

\section{Paladin数据存储处理平台部署}
由于不具备大规模实际容器云环境对几种调度算法进行测试，将在实验室小规模容器云平台Paladin上对算法性能进行测试，实验中使用多种分布式计算框架容器应用进行混合部署和性能测试。因此，首先需要搭建一个Paladin实验环境，Paladin是实验室开发的一个集海量数据存储管理，基于容器的多种分布式计算框架运行环境快速构建，存储和计算分离的多元化大数据数据处理平台。
\subsection{Paladin数据存储处理平台介绍}
Paladin是一个支持用户构建多种大数据计算框架、并提供海量数据存储与管理的平台。用户在平台中选择所需的计算与存储框架，如批处理(Hadoop、Spark)，流计算系统(Storm、Flink)、分布式的MPI、机器学习框架(Mahout、Tensorflow)、图计算系统（GridGraph、ReGraph)以及数据库系统(MongoDB、Mysql、Redis)等等。平台能够快速构建相应的多计算框架运行环境、并且根据具体的需求提供计算任务调度、海量数据存储，通过优化的容器云多维资源利用率均衡调度算法提升资源利用率和集群服务性能。

\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{paladin-structure}
	\caption{Paladin平台架构图}
\end{figure}

从Paladin架构图可以看出，最底层是基础设施部分，即整个集群的物理机和云主机部分，中间是支持上层的中间软件服务如分布式文件系统、Docker容器服务、OpenShift Origin容器云等，最上层提供海量数据存储管理和多种数据处理计算框架，顶层是Web服务层。整个架构中最为重要的两个部分数据存储管理模块和数据处理框架模块，数据管理存储提供用户数据上传、下载、管理、浏览、分享、挂在等一个分布式大数据管理存储系统。数据处理框架提供用户快速构建各种计算框架容器集群环境，根据实际需求配置资源、容器的动态增减变动等，该部分基于开源OpenShift Origin进行搭建，在任何一个容器中可以实现存储数据的挂载，实现存储和计算分离。当前计算框架除支持单应用的Nginx、Apache、PHP、MongoDB等，也支持大量分布式数据处理框架如Hadoop、SPark、Storm、Spark、TensorFlow以及实验室的图计算Regraph、GridGraph等数十种计算框架运行环境的快速部署。用户既可以简单用该系统进行大数据存储管理，也能快速构建大数据处理框架，实现海量数据处理，管理员对基础设施和存储节点，以及系统权限等进行管理。

\subsection{Paladin数据存储处理平台部署}
首先需要构建小规模的Paladin数据存储处理平台，构建一个容器云环境，该平台集成数据存储和管理。实验中采用6台物理机进行部署，一个主节点Matser，Master节点同时也是Node节点，四个Node节点和一个存储Master节点，由于该存储也需要对用户提供Web Service的服务，因此单独使用一个节点进行部署。将用户的存储管理服务和计算框架服务分离，分别部署在两个不同的物理机上，提升集群吞吐率。S-Master是分布式存储的Master节点，各物理节点的配置如下:
\begin{table}[H]
	\centering\dawu[1.3]
	\caption{Paladin平台物理机配置}
	\begin{tabular}{|p{1.8cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}|p{3cm}<{\centering}|} \hline
		\diagbox[innerwidth=1.8cm]{节点}{资源} & CPU核数 & 内存(G) & 磁盘(G) & 带宽(M) & IP \\ \hline
		Master & 4 & 16 & 1000 & 100 & 192.168.1.100  \\ \hline
		Node1 & 4 & 16 & 1000 & 100  & 192.168.1.101 \\ \hline
		Node2 & 4 & 16 & 1000 & 100  & 192.168.1.102 \\ \hline
		Node3 & 4 & 16 & 1000 & 100  & 192.168.1.103 \\ \hline
		Node4 & 4 & 16 & 1000 & 100  & 192.168.1.104 \\ \hline
		S-Matser & 4 & 8 & 500 & 100  & 192.168.1.110 \\ \hline
	\end{tabular}
\end{table}

各节点上需要安装的主要软件如下图所示，部署一个完整的容器云平台。在分布式计算框架部分Master节点上主要部署openshift-ansible及其依赖的各种软件包，用于安装OpenShift集群、openshift-origin集群、底层的Docker及其私有仓库、openshift-origin以来的一致性存储Etcd、实验室数据存储管理需要的哭护短paladin-client、经过优化更改后的origin-web、操作系统Centos 7.5。Master节点即作为容器云的主控节点，同时也是Node节点，在分布式存储中作为数据存储节点。S-Mastershi是分布式存储和管理的主控节点，使用Centos 6.9系统，安装Nginx服务、paladin-web-console对普通用户提供数据存储管理的web服务，同时也是数据节点。Node节点作为计算节点，安装Docker、openshift-origin集群子节点的需要的依赖软件如kublet、Haproxy等系列软件、同时还有Etcd和paladin-client、作为存储集群的数据节点，系统是Centos7.5。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics{paladin-platform}
	\caption{Paladin平台节点分布图}
\end{figure}

安装完成后，Paladin平台的计算框架部分，查询集群活跃节点如下:
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=8cm]{paladin-nodelist}
	\caption{Paladin安装成功计算节点部分}
\end{figure}

存储paladin-web-console部分，用户进行数据存储管理，用户在该部分可以进行海量数据的上传、下载、删除等管理，还可以通过paladin-client将数据以盘的形式挂在到物理节点或者容器中，在分布式计算框架容器应用中直接使用数据，实现存储和计算分离。
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=7cm]{paladin-web-console}
	\caption{Paladin数据存储管理服务}
\end{figure}

整个Paladin平台部署完成后，在该平台上开发部署各种大数据处理框架和容器应用，平台origin-web-console用户服务界面如下:
\begin{figure}[H] % use float package if you want it here
	\centering
	\includegraphics[width=13cm, height=7cm]{origin-web-console}
	\caption{Paladin数据处理框架部署服务}
\end{figure}

至此，整个Paladin大数据存储管理和计算框架快速部署容器云平台部署完成，实现了存储计算分离，用户既能进行大数据存储管理，实现普通的云存储系统，又能快速构建大数据处理容器应用框架，实现海量数据处理，为普通用户提供优秀的大数据容器云配套服务。

\section{多计算框架容器应用开发}









